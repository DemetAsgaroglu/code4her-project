{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bba34b-e8a4-4b5c-af44-b602f7576f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri klasÃ¶rÃ¼: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\n",
      "Ã‡Ä±ktÄ± dosyasÄ±: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\n",
      "6284_sayili_kanun.txt dosyasÄ±ndan 124 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "gumushane_el_kitabi.txt dosyasÄ±ndan 95 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "KADES_bilgilendirme.txt dosyasÄ±ndan 12 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "kadin_dayanisma_vakfi_rehber.txt dosyasÄ±ndan 22 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "mor_cati_rehber.txt dosyasÄ±ndan 393 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "sonim_yonetmelik.txt dosyasÄ±ndan 80 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "sÄ±das.txt dosyasÄ±ndan 34 kayÄ±t Ã§Ä±karÄ±ldÄ±.\n",
      "\n",
      "Toplam 760 kayÄ±t C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl dosyasÄ±na yazÄ±ldÄ±.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- KLASÃ–R YOLLARI ---\n",
    "DATA_DIR = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\"\n",
    "OUT_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "\n",
    "# --- VERÄ° AYIKLAMA FONKSÄ°YONU ---\n",
    "def parse_file(file_path: Path):\n",
    "    text = file_path.read_text(encoding=\"utf-8\")\n",
    "    \n",
    "    # \"Soru:\" ve \"## Soru:\" her iki formatÄ± da destekleyen regex\n",
    "    pattern = re.compile(\n",
    "        r\"(?:##\\s*)?Soru[:\\-â€“]\\s*(.*?)\\s*Cevap[:\\-â€“]\\s*(.*?)(?=\\n\\Soru[:\\-â€“]|\\n##\\s*Soru[:\\-â€“]|\\Z)\",\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    qa_pairs = []\n",
    "    for match in pattern.finditer(text):\n",
    "        question = match.group(1).strip().replace(\"\\n\", \" \")\n",
    "        answer = match.group(2).strip().replace(\"\\n\", \" \")\n",
    "        qa_pairs.append({\n",
    "            \"source\": file_path.name,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return qa_pairs\n",
    "\n",
    "# --- ANA FONKSÄ°YON ---\n",
    "def main():\n",
    "    out_path = Path(OUT_FILE)\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Veri klasÃ¶rÃ¼: {data_dir}\")\n",
    "    print(f\"Ã‡Ä±ktÄ± dosyasÄ±: {out_path}\")\n",
    "    \n",
    "    for p in data_dir.glob(\"*.txt\"):\n",
    "        parsed = parse_file(p)\n",
    "        all_data.extend(parsed)\n",
    "        print(f\"{p.name} dosyasÄ±ndan {len(parsed)} kayÄ±t Ã§Ä±karÄ±ldÄ±.\")\n",
    "    \n",
    "    # JSONL yaz\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in all_data:\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nToplam {len(all_data)} kayÄ±t {out_path} dosyasÄ±na yazÄ±ldÄ±.\")\n",
    "\n",
    "# --- Ã‡ALIÅTIR ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1664403-b705-44fb-b209-977fab737938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Ã¼retiliyor...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:13<00:00, 55.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 adet embedding Ã¼retildi ve kaydedildi:\n",
      "- Embedding dosyasÄ±: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\n",
      "- Metin dosyasÄ±: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Modeli yÃ¼kle\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "# Veri dosyasÄ±nÄ±n yolu\n",
    "input_file = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "output_embeddings = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "output_texts = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "# JSONL dosyasÄ±nÄ± oku\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        texts.append(record.get(\"text\") or record.get(\"content\") or \"\")\n",
    "\n",
    "# Embedding Ã¼retimi\n",
    "embeddings = []\n",
    "for text in tqdm(texts, desc=\"Embedding Ã¼retiliyor...\"):\n",
    "    emb = model.encode(text)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Embedding'leri ve metinleri kaydet\n",
    "np.save(output_embeddings, embeddings)\n",
    "\n",
    "with open(output_texts, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(embeddings)} adet embedding Ã¼retildi ve kaydedildi:\")\n",
    "print(f\"- Embedding dosyasÄ±: {output_embeddings}\")\n",
    "print(f\"- Metin dosyasÄ±: {output_texts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab61e2f-fe79-4efa-b7c2-a13e8f96d550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabc6d33-076e-4242-a76f-9dc57ecb0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam metin: 760\n",
      "Ã–rnek: ['Soru: Bu yÃ¶netmeliÄŸin amacÄ± nedir? [cite_start] Cevap: Bu yÃ¶netmeliÄŸin amacÄ±, ÅŸiddete uÄŸrayan veya ÅŸiddete uÄŸrama tehlikesi bulunan kiÅŸilerin korunmasÄ±, bu kiÅŸilere yÃ¶nelik ÅŸiddetin Ã¶nlenmesi ve ÅŸiddet uygulayanlar hakkÄ±nda alÄ±nacak tedbirlere iliÅŸkin usul ve esaslarÄ± dÃ¼zenlemektir[cite: 5].', 'Soru: 6284 sayÄ±lÄ± kanun kimleri korur? Sadece kadÄ±nlar iÃ§in mi? Cevap: HayÄ±r, kanun sadece kadÄ±nlarÄ± deÄŸil; [cite_start]ÅŸiddete uÄŸrayan veya ÅŸiddete uÄŸrama tehlikesi bulunan kadÄ±nlarÄ±, Ã§ocuklarÄ±, aile bireylerini ve tek taraflÄ± Ä±srarlÄ± takip maÄŸduru olan kiÅŸileri korur[cite: 5].', 'Soru: Bu kanun ÅŸiddet uygulayanlar hakkÄ±nda da bir dÃ¼zenleme iÃ§eriyor mu? [cite_start] Cevap: Evet, kanun aynÄ± zamanda ÅŸiddet uygulayan veya uygulama ihtimali olan kiÅŸiler hakkÄ±nda ÅŸiddetin Ã¶nlenmesine yÃ¶nelik tedbirleri ve bu tedbirlerin nasÄ±l alÄ±nacaÄŸÄ±nÄ± da kapsar[cite: 5].  ---  # Konu: Temel Kavramlar ve Yasal TanÄ±mlar']\n",
      "Embedding Ã¼retiliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760/760 [00:35<00:00, 21.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "760 adet embedding Ã¼retildi ve kaydedildi:\n",
      "- Embedding dosyasÄ±: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\n",
      "- Metin dosyasÄ±: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dosya yollarÄ±\n",
    "JSONL_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "EMB_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "TEXT_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "\n",
    "# Modeli yÃ¼kle\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "texts = []\n",
    "\n",
    "# JSONL dosyasÄ±nÄ± oku ve question+answer'i birleÅŸtir\n",
    "with open(JSONL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        question = record.get(\"question\", \"\").strip()\n",
    "        answer = record.get(\"answer\", \"\").strip()\n",
    "        if question and answer:\n",
    "            # Tek bir metin halinde birleÅŸtir\n",
    "            text = f\"Soru: {question} Cevap: {answer}\"\n",
    "            texts.append(text)\n",
    "\n",
    "print(f\"Toplam metin: {len(texts)}\")\n",
    "print(\"Ã–rnek:\", texts[:3])\n",
    "\n",
    "# Embedding Ã¼retimi\n",
    "embeddings = []\n",
    "print(\"Embedding Ã¼retiliyor...\")\n",
    "for t in tqdm(texts):\n",
    "    emb = model.encode(t)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# DosyalarÄ± kaydet\n",
    "np.save(EMB_PATH, embeddings)\n",
    "with open(TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n{len(texts)} adet embedding Ã¼retildi ve kaydedildi:\")\n",
    "print(f\"- Embedding dosyasÄ±: {EMB_PATH}\")\n",
    "print(f\"- Metin dosyasÄ±: {TEXT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3378c537-ae69-43f4-80b5-1909d22cf898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index oluÅŸturuldu ve kaydedildi: C:\\Users\\hukus\\Downloads\\Code4Her\\faiss_index.index\n",
      "\n",
      "ğŸ” En benzer kayÄ±tlar:\n",
      "\n",
      "Skor: 0.8858\n",
      "Metin: Soru: SÄ±ÄŸÄ±naÄŸa nasÄ±l baÅŸvurulur ve baÅŸvuruyu kim yapmalÄ±dÄ±r? Cevap: SÄ±ÄŸÄ±nak baÅŸvurusunu kendiniz yapmalÄ±sÄ±nÄ±z. BakanlÄ±ÄŸa baÄŸlÄ± bir sÄ±ÄŸÄ±naÄŸa yerleÅŸmek iÃ§in Ä°l MÃ¼dÃ¼rlÃ¼ÄŸÃ¼'ne, ÅÃ–NÄ°M'e, Polise/Jandarmaya ya da KaymakamlÄ±ÄŸa/ValiliÄŸe baÅŸvurabilirsiniz....\n",
      "\n",
      "Skor: 0.8489\n",
      "Metin: Soru: SÄ±ÄŸÄ±naÄŸa nasÄ±l giderim? UlaÅŸÄ±m saÄŸlanÄ±yor mu? Cevap: Evet, ÅÃ–NÄ°M tarafÄ±ndan il iÃ§i ve il dÄ±ÅŸÄ± nakillerde ulaÅŸÄ±m iÃ§in araÃ§ tahsis edilir. AyrÄ±ca ulaÅŸÄ±m giderleriniz ile diÄŸer zorunlu giderleriniz de karÅŸÄ±lanÄ±r. Hayati tehlikenizin bulunmasÄ± halinde ise yerleÅŸtirileceÄŸiniz yere kolluk (polis/jandarma) refakat eder....\n",
      "\n",
      "Skor: 0.8488\n",
      "Metin: Soru: BakanlÄ±ÄŸa baÄŸlÄ± sÄ±ÄŸÄ±naklara kabul iÃ§in yaÅŸ sÄ±nÄ±rÄ± nedir? Cevap: SÄ±ÄŸÄ±naÄŸa kabul edilmek iÃ§in 18 yaÅŸÄ±ndan bÃ¼yÃ¼k, 60 yaÅŸÄ±ndan kÃ¼Ã§Ã¼k olmanÄ±z gerekir....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dosya yollarÄ±\n",
    "EMB_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "TEXT_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "FAISS_INDEX_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\faiss_index.index\"\n",
    "\n",
    "# Embedding ve metinleri yÃ¼kle\n",
    "embeddings = np.load(EMB_PATH)\n",
    "with open(TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "# Embedding boyutu\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# FAISS index oluÅŸtur (cosine similarity iÃ§in normalize ediyoruz)\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product\n",
    "faiss.normalize_L2(embeddings)       # Cosine similarity iÃ§in normalize\n",
    "index.add(embeddings)\n",
    "\n",
    "# Index'i kaydet\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "print(f\"FAISS index oluÅŸturuldu ve kaydedildi: {FAISS_INDEX_PATH}\")\n",
    "\n",
    "# Arama fonksiyonu\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "def search_similar(query, top_k=3):\n",
    "    query_vec = model.encode([query]).astype(\"float32\")\n",
    "    faiss.normalize_L2(query_vec)\n",
    "    D, I = index.search(query_vec, top_k)  # D = skorlar, I = indeksler\n",
    "    print(\"\\nğŸ” En benzer kayÄ±tlar:\\n\")\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        print(f\"Skor: {score:.4f}\")\n",
    "        print(f\"Metin: {texts[idx][:500]}...\\n\")  # Ä°lk 500 karakteri gÃ¶ster\n",
    "\n",
    "# Test\n",
    "search_similar(\"SÄ±ÄŸÄ±naÄŸa kimler baÅŸvurabilir?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10b7a3-62a6-455d-827e-64704696b7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
