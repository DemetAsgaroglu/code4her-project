{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bba34b-e8a4-4b5c-af44-b602f7576f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri klasörü: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\n",
      "Çıktı dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\n",
      "6284_sayili_kanun.txt dosyasından 124 kayıt çıkarıldı.\n",
      "gumushane_el_kitabi.txt dosyasından 95 kayıt çıkarıldı.\n",
      "KADES_bilgilendirme.txt dosyasından 12 kayıt çıkarıldı.\n",
      "kadin_dayanisma_vakfi_rehber.txt dosyasından 22 kayıt çıkarıldı.\n",
      "mor_cati_rehber.txt dosyasından 393 kayıt çıkarıldı.\n",
      "sonim_yonetmelik.txt dosyasından 80 kayıt çıkarıldı.\n",
      "sıdas.txt dosyasından 34 kayıt çıkarıldı.\n",
      "\n",
      "Toplam 760 kayıt C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl dosyasına yazıldı.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- KLASÖR YOLLARI ---\n",
    "DATA_DIR = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\"\n",
    "OUT_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "\n",
    "# --- VERİ AYIKLAMA FONKSİYONU ---\n",
    "def parse_file(file_path: Path):\n",
    "    text = file_path.read_text(encoding=\"utf-8\")\n",
    "    \n",
    "    # \"Soru:\" ve \"## Soru:\" her iki formatı da destekleyen regex\n",
    "    pattern = re.compile(\n",
    "        r\"(?:##\\s*)?Soru[:\\-–]\\s*(.*?)\\s*Cevap[:\\-–]\\s*(.*?)(?=\\n\\Soru[:\\-–]|\\n##\\s*Soru[:\\-–]|\\Z)\",\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    qa_pairs = []\n",
    "    for match in pattern.finditer(text):\n",
    "        question = match.group(1).strip().replace(\"\\n\", \" \")\n",
    "        answer = match.group(2).strip().replace(\"\\n\", \" \")\n",
    "        qa_pairs.append({\n",
    "            \"source\": file_path.name,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return qa_pairs\n",
    "\n",
    "# --- ANA FONKSİYON ---\n",
    "def main():\n",
    "    out_path = Path(OUT_FILE)\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Veri klasörü: {data_dir}\")\n",
    "    print(f\"Çıktı dosyası: {out_path}\")\n",
    "    \n",
    "    for p in data_dir.glob(\"*.txt\"):\n",
    "        parsed = parse_file(p)\n",
    "        all_data.extend(parsed)\n",
    "        print(f\"{p.name} dosyasından {len(parsed)} kayıt çıkarıldı.\")\n",
    "    \n",
    "    # JSONL yaz\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in all_data:\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nToplam {len(all_data)} kayıt {out_path} dosyasına yazıldı.\")\n",
    "\n",
    "# --- ÇALIŞTIR ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1664403-b705-44fb-b209-977fab737938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding üretiliyor...: 100%|██████████| 760/760 [00:13<00:00, 55.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 adet embedding üretildi ve kaydedildi:\n",
      "- Embedding dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\n",
      "- Metin dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Modeli yükle\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "# Veri dosyasının yolu\n",
    "input_file = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "output_embeddings = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "output_texts = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "# JSONL dosyasını oku\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        texts.append(record.get(\"text\") or record.get(\"content\") or \"\")\n",
    "\n",
    "# Embedding üretimi\n",
    "embeddings = []\n",
    "for text in tqdm(texts, desc=\"Embedding üretiliyor...\"):\n",
    "    emb = model.encode(text)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Embedding'leri ve metinleri kaydet\n",
    "np.save(output_embeddings, embeddings)\n",
    "\n",
    "with open(output_texts, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(embeddings)} adet embedding üretildi ve kaydedildi:\")\n",
    "print(f\"- Embedding dosyası: {output_embeddings}\")\n",
    "print(f\"- Metin dosyası: {output_texts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab61e2f-fe79-4efa-b7c2-a13e8f96d550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\hukus\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabc6d33-076e-4242-a76f-9dc57ecb0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam metin: 760\n",
      "Örnek: ['Soru: Bu yönetmeliğin amacı nedir? [cite_start] Cevap: Bu yönetmeliğin amacı, şiddete uğrayan veya şiddete uğrama tehlikesi bulunan kişilerin korunması, bu kişilere yönelik şiddetin önlenmesi ve şiddet uygulayanlar hakkında alınacak tedbirlere ilişkin usul ve esasları düzenlemektir[cite: 5].', 'Soru: 6284 sayılı kanun kimleri korur? Sadece kadınlar için mi? Cevap: Hayır, kanun sadece kadınları değil; [cite_start]şiddete uğrayan veya şiddete uğrama tehlikesi bulunan kadınları, çocukları, aile bireylerini ve tek taraflı ısrarlı takip mağduru olan kişileri korur[cite: 5].', 'Soru: Bu kanun şiddet uygulayanlar hakkında da bir düzenleme içeriyor mu? [cite_start] Cevap: Evet, kanun aynı zamanda şiddet uygulayan veya uygulama ihtimali olan kişiler hakkında şiddetin önlenmesine yönelik tedbirleri ve bu tedbirlerin nasıl alınacağını da kapsar[cite: 5].  ---  # Konu: Temel Kavramlar ve Yasal Tanımlar']\n",
      "Embedding üretiliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [00:35<00:00, 21.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "760 adet embedding üretildi ve kaydedildi:\n",
      "- Embedding dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\n",
      "- Metin dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dosya yolları\n",
    "JSONL_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
    "EMB_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "TEXT_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "\n",
    "# Modeli yükle\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "texts = []\n",
    "\n",
    "# JSONL dosyasını oku ve question+answer'i birleştir\n",
    "with open(JSONL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        question = record.get(\"question\", \"\").strip()\n",
    "        answer = record.get(\"answer\", \"\").strip()\n",
    "        if question and answer:\n",
    "            # Tek bir metin halinde birleştir\n",
    "            text = f\"Soru: {question} Cevap: {answer}\"\n",
    "            texts.append(text)\n",
    "\n",
    "print(f\"Toplam metin: {len(texts)}\")\n",
    "print(\"Örnek:\", texts[:3])\n",
    "\n",
    "# Embedding üretimi\n",
    "embeddings = []\n",
    "print(\"Embedding üretiliyor...\")\n",
    "for t in tqdm(texts):\n",
    "    emb = model.encode(t)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# Dosyaları kaydet\n",
    "np.save(EMB_PATH, embeddings)\n",
    "with open(TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n{len(texts)} adet embedding üretildi ve kaydedildi:\")\n",
    "print(f\"- Embedding dosyası: {EMB_PATH}\")\n",
    "print(f\"- Metin dosyası: {TEXT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3378c537-ae69-43f4-80b5-1909d22cf898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index oluşturuldu ve kaydedildi: C:\\Users\\hukus\\Downloads\\Code4Her\\faiss_index.index\n",
      "\n",
      "🔍 En benzer kayıtlar:\n",
      "\n",
      "Skor: 0.8858\n",
      "Metin: Soru: Sığınağa nasıl başvurulur ve başvuruyu kim yapmalıdır? Cevap: Sığınak başvurusunu kendiniz yapmalısınız. Bakanlığa bağlı bir sığınağa yerleşmek için İl Müdürlüğü'ne, ŞÖNİM'e, Polise/Jandarmaya ya da Kaymakamlığa/Valiliğe başvurabilirsiniz....\n",
      "\n",
      "Skor: 0.8489\n",
      "Metin: Soru: Sığınağa nasıl giderim? Ulaşım sağlanıyor mu? Cevap: Evet, ŞÖNİM tarafından il içi ve il dışı nakillerde ulaşım için araç tahsis edilir. Ayrıca ulaşım giderleriniz ile diğer zorunlu giderleriniz de karşılanır. Hayati tehlikenizin bulunması halinde ise yerleştirileceğiniz yere kolluk (polis/jandarma) refakat eder....\n",
      "\n",
      "Skor: 0.8488\n",
      "Metin: Soru: Bakanlığa bağlı sığınaklara kabul için yaş sınırı nedir? Cevap: Sığınağa kabul edilmek için 18 yaşından büyük, 60 yaşından küçük olmanız gerekir....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dosya yolları\n",
    "EMB_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
    "TEXT_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
    "FAISS_INDEX_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\faiss_index.index\"\n",
    "\n",
    "# Embedding ve metinleri yükle\n",
    "embeddings = np.load(EMB_PATH)\n",
    "with open(TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "# Embedding boyutu\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# FAISS index oluştur (cosine similarity için normalize ediyoruz)\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product\n",
    "faiss.normalize_L2(embeddings)       # Cosine similarity için normalize\n",
    "index.add(embeddings)\n",
    "\n",
    "# Index'i kaydet\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "print(f\"FAISS index oluşturuldu ve kaydedildi: {FAISS_INDEX_PATH}\")\n",
    "\n",
    "# Arama fonksiyonu\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "def search_similar(query, top_k=3):\n",
    "    query_vec = model.encode([query]).astype(\"float32\")\n",
    "    faiss.normalize_L2(query_vec)\n",
    "    D, I = index.search(query_vec, top_k)  # D = skorlar, I = indeksler\n",
    "    print(\"\\n🔍 En benzer kayıtlar:\\n\")\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        print(f\"Skor: {score:.4f}\")\n",
    "        print(f\"Metin: {texts[idx][:500]}...\\n\")  # İlk 500 karakteri göster\n",
    "\n",
    "# Test\n",
    "search_similar(\"Sığınağa kimler başvurabilir?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10b7a3-62a6-455d-827e-64704696b7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
