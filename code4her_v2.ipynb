{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31bba34b-e8a4-4b5c-af44-b602f7576f14",
      "metadata": {
        "id": "31bba34b-e8a4-4b5c-af44-b602f7576f14",
        "outputId": "acaef746-2265-4727-9fd4-821d6f70288e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri klasörü: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\n",
            "Çıktı dosyası: C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\n",
            "\n",
            "Toplam 0 kayıt C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl dosyasına yazıldı.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# --- KLASÖR YOLLARI ---\n",
        "DATA_DIR = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledgw_base\"\n",
        "OUT_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
        "\n",
        "# --- VERİ AYIKLAMA FONKSİYONU ---\n",
        "def parse_file(file_path: Path):\n",
        "    text = file_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    # \"Soru:\" ve \"## Soru:\" her iki formatı da destekleyen regex\n",
        "    pattern = re.compile(\n",
        "        r\"(?:##\\s*)?Soru[:\\-–]\\s*(.*?)\\s*Cevap[:\\-–]\\s*(.*?)(?=\\n\\Soru[:\\-–]|\\n##\\s*Soru[:\\-–]|\\Z)\",\n",
        "        re.DOTALL | re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    qa_pairs = []\n",
        "    for match in pattern.finditer(text):\n",
        "        question = match.group(1).strip().replace(\"\\n\", \" \")\n",
        "        answer = match.group(2).strip().replace(\"\\n\", \" \")\n",
        "        qa_pairs.append({\n",
        "            \"source\": file_path.name,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer\n",
        "        })\n",
        "    return qa_pairs\n",
        "\n",
        "# --- ANA FONKSİYON ---\n",
        "def main():\n",
        "    out_path = Path(OUT_FILE)\n",
        "    data_dir = Path(DATA_DIR)\n",
        "\n",
        "    all_data = []\n",
        "    print(f\"Veri klasörü: {data_dir}\")\n",
        "    print(f\"Çıktı dosyası: {out_path}\")\n",
        "\n",
        "    for p in data_dir.glob(\"*.txt\"):\n",
        "        parsed = parse_file(p)\n",
        "        all_data.extend(parsed)\n",
        "        print(f\"{p.name} dosyasından {len(parsed)} kayıt çıkarıldı.\")\n",
        "\n",
        "    # JSONL yaz\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
        "        for item in all_data:\n",
        "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"\\nToplam {len(all_data)} kayıt {out_path} dosyasına yazıldı.\")\n",
        "\n",
        "# --- ÇALIŞTIR ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1664403-b705-44fb-b209-977fab737938",
      "metadata": {
        "id": "f1664403-b705-44fb-b209-977fab737938"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Modeli yükle\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "# Veri dosyasının yolu\n",
        "input_file = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
        "output_embeddings = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
        "output_texts = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
        "\n",
        "texts = []\n",
        "\n",
        "# JSONL dosyasını oku\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        record = json.loads(line)\n",
        "        texts.append(record.get(\"text\") or record.get(\"content\") or \"\")\n",
        "\n",
        "# Embedding üretimi\n",
        "embeddings = []\n",
        "for text in tqdm(texts, desc=\"Embedding üretiliyor...\"):\n",
        "    emb = model.encode(text)\n",
        "    embeddings.append(emb)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Embedding'leri ve metinleri kaydet\n",
        "np.save(output_embeddings, embeddings)\n",
        "\n",
        "with open(output_texts, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"{len(embeddings)} adet embedding üretildi ve kaydedildi:\")\n",
        "print(f\"- Embedding dosyası: {output_embeddings}\")\n",
        "print(f\"- Metin dosyası: {output_texts}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab61e2f-fe79-4efa-b7c2-a13e8f96d550",
      "metadata": {
        "scrolled": true,
        "id": "3ab61e2f-fe79-4efa-b7c2-a13e8f96d550"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabc6d33-076e-4242-a76f-9dc57ecb0ce2",
      "metadata": {
        "id": "eabc6d33-076e-4242-a76f-9dc57ecb0ce2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dosya yolları\n",
        "JSONL_FILE = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_dataset.jsonl\"\n",
        "EMB_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_embeddings.npy\"\n",
        "TEXT_PATH = r\"C:\\Users\\hukus\\Downloads\\Code4Her\\knowledge_texts.json\"\n",
        "\n",
        "# Modeli yükle\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "texts = []\n",
        "\n",
        "# JSONL dosyasını oku ve question+answer'i birleştir\n",
        "with open(JSONL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        record = json.loads(line)\n",
        "        question = record.get(\"question\", \"\").strip()\n",
        "        answer = record.get(\"answer\", \"\").strip()\n",
        "        if question and answer:\n",
        "            # Tek bir metin halinde birleştir\n",
        "            text = f\"Soru: {question} Cevap: {answer}\"\n",
        "            texts.append(text)\n",
        "\n",
        "print(f\"Toplam metin: {len(texts)}\")\n",
        "print(\"Örnek:\", texts[:3])\n",
        "\n",
        "# Embedding üretimi\n",
        "embeddings = []\n",
        "print(\"Embedding üretiliyor...\")\n",
        "for t in tqdm(texts):\n",
        "    emb = model.encode(t)\n",
        "    embeddings.append(emb)\n",
        "\n",
        "embeddings = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "# Dosyaları kaydet\n",
        "np.save(EMB_PATH, embeddings)\n",
        "with open(TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n{len(texts)} adet embedding üretildi ve kaydedildi:\")\n",
        "print(f\"- Embedding dosyası: {EMB_PATH}\")\n",
        "print(f\"- Metin dosyası: {TEXT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Dosya yolları\n",
        "EMB_PATH = \"/content/knowledge_embeddings.npy\"\n",
        "TEXT_PATH = \"/content/knowledge_texts.json\"\n",
        "FAISS_INDEX_PATH = \"/content/faiss_index.index\"\n",
        "\n",
        "# Embedding ve metinleri yükle\n",
        "embeddings = np.load(EMB_PATH)\n",
        "with open(TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = json.load(f)\n",
        "\n",
        "# FAISS index oluştur\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "faiss.normalize_L2(embeddings)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Index'i kaydet\n",
        "faiss.write_index(index, FAISS_INDEX_PATH)\n",
        "print(f\"FAISS index oluşturuldu ve kaydedildi: {FAISS_INDEX_PATH}\")\n",
        "\n",
        "# Arama fonksiyonu\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "def search_similar(query, top_k=3):\n",
        "    query_vec = model.encode([query]).astype(\"float32\")\n",
        "    faiss.normalize_L2(query_vec)\n",
        "    D, I = index.search(query_vec, top_k)\n",
        "    print(\"\\n En benzer kayıtlar:\\n\")\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        print(f\"Skor: {score:.4f}\")\n",
        "        print(f\"Metin: {texts[idx][:500]}...\\n\")\n",
        "\n",
        "# Test\n",
        "search_similar(\"Sığınağa kimler başvurabilir?\")\n"
      ],
      "metadata": {
        "id": "UF_BTV128vxJ"
      },
      "id": "UF_BTV128vxJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y langchain langchain-huggingface langchain-community\n",
        "!pip install -U langchain-huggingface\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OpAp_C_ERdMA"
      },
      "id": "OpAp_C_ERdMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain faiss-cpu sentence-transformers transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4cnb05NLWWA2"
      },
      "id": "4cnb05NLWWA2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.331 sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zjIfaOyQY_2d"
      },
      "id": "zjIfaOyQY_2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index.d)  # FAISS index’in embedding boyutu\n"
      ],
      "metadata": {
        "id": "-CX5BX3Tg1Vq"
      },
      "id": "-CX5BX3Tg1Vq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = embedding_model.embed_query(\"test\")\n",
        "print(len(vec))  # embedding boyutu\n"
      ],
      "metadata": {
        "id": "FXTOOVcqg_hX"
      },
      "id": "FXTOOVcqg_hX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "9B7O_RV2hMJd"
      },
      "id": "9B7O_RV2hMJd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tiHtr2P38AM5"
      },
      "id": "tiHtr2P38AM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_b3wilIx-MnK"
      },
      "id": "_b3wilIx-MnK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OhUfEL3T-ULI"
      },
      "id": "OhUfEL3T-ULI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y langchain langchain-core langchain-text-splitters langchain-community langchain-classic\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W-_6TluM_CvE"
      },
      "id": "W-_6TluM_CvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==1.0.0\n",
        "!pip install langchain-community==0.4\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AkxDINyt_GIu"
      },
      "id": "AkxDINyt_GIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JK5eK6TvAYHc"
      },
      "id": "JK5eK6TvAYHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import json\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.docstore import InMemoryDocstore\n",
        "from langchain.schema.document import Document  # <-- doğru import\n",
        "\n",
        "# Embedding modeli\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "# FAISS index'i yükle\n",
        "index = faiss.read_index(\"/content/faiss_index.index\")\n",
        "\n",
        "# Textleri yükle\n",
        "with open(\"/content/knowledge_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts_data = json.load(f)\n",
        "\n",
        "# Dokümanları Document objesine çevir ve dict oluştur\n",
        "docs_dict = {str(i): Document(page_content=t) for i, t in enumerate(texts_data)}\n",
        "\n",
        "# InMemoryDocstore oluştur\n",
        "docstore = InMemoryDocstore(docs_dict)\n",
        "\n",
        "# Index ile docstore id eşlemesini oluştur\n",
        "index_to_docstore_id = list(docs_dict.keys())\n",
        "\n",
        "# FAISS vectorstore objesi oluştur\n",
        "vectorstore = FAISS(\n",
        "    index=index,\n",
        "    embedding_function=embedding_model,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "1Md_-RGYDCjl"
      },
      "id": "1Md_-RGYDCjl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==1.0.0\n",
        "!pip install langchain-community==0.4\n",
        "!pip install langchain-huggingface\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "cUROilGZEYyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "610c439d-9464-4078-f778-f8c4a6965503"
      },
      "id": "cUROilGZEYyP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==1.0.0 in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain==1.0.0) (1.0.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain==1.0.0) (1.0.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==1.0.0) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: langchain-community==0.4 in /usr/local/lib/python3.12/dist-packages (0.4)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (1.0.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (0.4.37)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community==0.4) (1.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community==0.4) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-community==0.4) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-community==0.4) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-community==0.4) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community==0.4) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community==0.4) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community==0.4) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-community==0.4) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community==0.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community==0.4) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4) (1.3.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.35.3)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.4.37)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (2.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document  #\n",
        "\n",
        "print(\" Kurulum başarılı! FAISS + LangChain RAG ortamı hazır.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS5G-WflFa_C",
        "outputId": "aec86340-5537-4585-8717-f950a7f30a02"
      },
      "id": "iS5G-WflFa_C",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Kurulum başarılı! FAISS + LangChain RAG ortamı hazır.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(\" başarıyla yüklendi!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_VwFlzHa66",
        "outputId": "90772ea1-8d56-4391-d658-58ae075f8e10"
      },
      "id": "nh_VwFlzHa66",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " başarıyla yüklendi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import json\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.docstore import InMemoryDocstore\n",
        "from langchain_core.documents import Document  # ✅ yeni sürümde doğru import\n",
        "\n",
        "# Embedding modeli\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "# FAISS index yükleme\n",
        "index = faiss.read_index(\"/content/faiss_index.index\")\n",
        "\n",
        "# Textleri yükle\n",
        "with open(\"/content/knowledge_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts_data = json.load(f)\n",
        "\n",
        "# Dokümanları Document objesine çevir\n",
        "docs_dict = {str(i): Document(page_content=t) for i, t in enumerate(texts_data)}\n",
        "\n",
        "# InMemoryDocstore oluştur\n",
        "docstore = InMemoryDocstore(docs_dict)\n",
        "\n",
        "# Index-to-docstore ID listesi\n",
        "index_to_docstore_id = list(docs_dict.keys())\n",
        "\n",
        "# FAISS vectorstore oluştur\n",
        "vectorstore = FAISS(\n",
        "    index=index,\n",
        "    embedding_function=embedding_model,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n",
        "\n",
        "print(\"FAISS vektör veritabanı başarıyla yüklendi\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0dxSUS2IcfZ",
        "outputId": "62b9bae2-62a2-4650-dfb4-46360a0d23af"
      },
      "id": "A0dxSUS2IcfZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS vektör veritabanı başarıyla yüklendi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_rag(query):\n",
        "\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"Kontekst bilgilerini kullanarak soruyu yanıtla.\n",
        "\n",
        "Kontekst:\n",
        "{context}\n",
        "\n",
        "Soru: {query}\n",
        "Cevap:\"\"\"\n",
        "\n",
        "    result = pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "    if \"Cevap:\" in result:\n",
        "        result = result.split(\"Cevap:\")[-1].strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test et\n",
        "soru = \"KADES uygulaması nedir?\"\n",
        "cevap = query_rag(soru)\n",
        "print(\"Soru:\", soru)\n",
        "print(\"\\nCevap:\", cevap)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Ti3ppuIrGo",
        "outputId": "2a8e95b8-43d6-492b-8c52-2759013b10b5"
      },
      "id": "c3Ti3ppuIrGo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soru: KADES uygulaması nedir?\n",
            "\n",
            "Cevap: KADES uygulaması, Türkiye'de yaşayan ve tehdit altında olduğunu hisseden veya şiddet görme ihtimali olan tüm kadınlar için kullanıma sunulmuştur. Eşinden, aile üyelerinden veya herhangi bir kişiden şiddet gören ya da tehdit altında olan kadınlar bu uygulamayı kullanabilir. Uygulama, sadece kadın kullanıcılar için geliştirilmiştir. Bu uygulamada, kadınların kendilerine hakları ve özgürlükten etkili olmak için çeşitli politikaları sağlayacaktır. Ayrıca, kendi haklarını korumak ve güvenli bir şekilde yapabilecek hale gelebilir. KADES, şiddet mağdurunun ve genellikle tehdit altında olduğu için hedeflendirdiği gibi, özellikle kadınların kendilerine hakları ve özgürlüğü için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlüğünü korumak ve güvenli bir şekilde yapabilecek hale gelebilir. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgürlükleri ve güvenli bir şekilde korumak için uygulanır. Bu uygulamada, kadınların kendilerine hakları ve özgü\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__0a7WEoJS59"
      },
      "id": "__0a7WEoJS59",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}