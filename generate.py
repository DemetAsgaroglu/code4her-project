from transformers import AutoTokenizer, AutoModelForCausalLM
from retrieval import retrieve_documents
import torch

# --- MODELÄ° YÃœKLE (CPU) ---
model_name = "tiiuae/Falcon-E-3B-Instruct"  # Daha kÃ¼Ã§Ã¼k model, CPU uyumlu
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map=None)  # CPU kullanÄ±mÄ±nda None

# --- SORU AL VE RETRIEVE ---
query = input("Bir soru sor: ")
docs = retrieve_documents(query)

# --- RETRIEVAL SONUÃ‡LARINI BÄ°RLEÅžTÄ°R ---
context = "\n".join([f"{d['question']} â†’ {d['answer']}" for d in docs])
prompt = f"Soru: {query}\nBilgi: {context}\nCevap:"

# --- MODELLE ÃœRETÄ°M ---
inputs = tokenizer(prompt, return_tensors="pt")

# âš¡ CPU modeli iÃ§in token_type_ids varsa kaldÄ±r
if "token_type_ids" in inputs:
    del inputs["token_type_ids"]

# CPUâ€™da RAM kullanÄ±mÄ±nÄ± azaltmak iÃ§in no_grad kullan
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nðŸ”¹ Cevap:\n", answer)
